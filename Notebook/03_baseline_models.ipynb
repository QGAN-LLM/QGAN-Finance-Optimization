# Baseline Models with Ethical Constraints
# Demonstrates traditional ML models for comparison
# while maintaining Data Minimization and Purpose Limitation

import sys
sys.path.append('..')

import pandas as pd
import numpy as np
from pathlib import Path
import yaml
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ML imports
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier

# Custom imports
from src.models.baseline import BaselineModelTrainer
from src.models.llm_finance import FinancialLLM
from src.evaluation.metrics import EthicalMetricsCalculator
from src.utils.logger import setup_logger

print("=" * 80)
print("BASELINE MODELS WITH ETHICAL CONSTRAINTS")
print("=" * 80)
print("Principle: Compare traditional models within ethical bounds")
print("Purpose: Establish benchmarks for QGAN comparison\n")

# Setup
logger = setup_logger('baseline_models')
with open('../configs/model_config.yaml', 'r') as f:
    model_config = yaml.safe_load(f)

# ============================================================================
# 1. LOAD PREPROCESSED DATA
# ============================================================================
print("\n1. LOADING PREPROCESSED DATA")
print("-" * 40)

data_dir = Path("../data/processed")
train_data = pd.read_parquet(data_dir / "train_data.parquet")
test_data = pd.read_parquet(data_dir / "test_data.parquet")

print(f"Training data shape: {train_data.shape}")
print(f"Test data shape: {test_data.shape}")
print(f"Features: {list(train_data.columns[:10])}...")

# Define target (binary: price up/down next period)
train_data['target'] = (train_data['mid_price'].shift(-1) > train_data['mid_price']).astype(int)
test_data['target'] = (test_data['mid_price'].shift(-1) > test_data['mid_price']).astype(int)

# Remove last row with NaN target
train_data = train_data.dropna(subset=['target'])
test_data = test_data.dropna(subset=['target'])

print(f"\nClass distribution:")
print(f" Training - Up: {(train_data['target'] == 1).sum():,} "
      f"({(train_data['target'] == 1).mean():.1%}), "
      f"Down: {(train_data['target'] == 0).sum():,} "
      f"({(train_data['target'] == 0).mean():.1%})")

# ============================================================================
# 2. FEATURE SELECTION (Data Minimization)
# ============================================================================
print("\n2. FEATURE SELECTION (Data Minimization Principle)")
print("-" * 40)

# Select only necessary features
feature_cols = [
    'ret_1', 'ret_2', 'ret_3', # Recent returns
    'ta_rsi_14', 'ta_macd', # Technical indicators
    'vol_atr_14', 'vol_realized_20', # Volatility
    'time_hour', 'time_is_weekend' # Time features
]

# Validate features are available
available_features = [f for f in feature_cols if f in train_data.columns]
print(f"Selected {len(available_features)} features for modeling")
print(f"Features: {available_features}")

X_train = train_data[available_features].values
y_train = train_data['target'].values
X_test = test_data[available_features].values
y_test = test_data['target'].values

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nTraining set: {X_train.shape}")
print(f"Test set: {X_test.shape}")

# ============================================================================
# 3. TRAIN BASELINE MODELS
# ============================================================================
print("\n3. TRAINING BASELINE MODELS")
print("-" * 40)

baseline_trainer = BaselineModelTrainer(model_config)

# Dictionary to store results
results = {}

# 3.1 Random Forest
print("\nTraining Random Forest...")
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=20,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train_scaled, y_train)
rf_pred = rf_model.predict(X_test_scaled)
rf_proba = rf_model.predict_proba(X_test_scaled)[:, 1]

results['RandomForest'] = {
    'model': rf_model,
    'predictions': rf_pred,
    'probabilities': rf_proba,
    'accuracy': accuracy_score(y_test, rf_pred),
    'f1': f1_score(y_test, rf_pred)
}

print(f" Accuracy: {results['RandomForest']['accuracy']:.3f}")
print(f" F1-Score: {results['RandomForest']['f1']:.3f}")

# 3.2 XGBoost
print("\nTraining XGBoost...")
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

xgb_model.fit(X_train_scaled, y_train)
xgb_pred = xgb_model.predict(X_test_scaled)
xgb_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]

results['XGBoost'] = {
    'model': xgb_model,
    'predictions': xgb_pred,
    'probabilities': xgb_proba,
    'accuracy': accuracy_score(y_test, xgb_pred),
    'f1': f1_score(y_test, xgb_pred)
}

print(f" Accuracy: {results['XGBoost']['accuracy']:.3f}")
print(f" F1-Score: {results['XGBoost']['f1']:.3f}")

# 3.3 LightGBM
print("\nTraining LightGBM...")
lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)

lgb_model.fit(X_train_scaled, y_train)
lgb_pred = lgb_model.predict(X_test_scaled)
lgb_proba = lgb_model.predict_proba(X_test_scaled)[:, 1]

results['LightGBM'] = {
    'model': lgb_model,
    'predictions': lgb_pred,
    'probabilities': lgb_proba,
    'accuracy': accuracy_score(y_test, lgb_pred),
    'f1': f1_score(y_test, lgb_pred)
}

print(f" Accuracy: {results['LightGBM']['accuracy']:.3f}")
print(f" F1-Score: {results['LightGBM']['f1']:.3f}")

# ============================================================================
# 4. FINANCIAL LLM BASELINE (Simplified)
# ============================================================================
print("\n4. FINANCIAL LLM BASELINE (Purpose-Limited)")
print("-" * 40)

# Note: Full LLM would require extensive resources
# Here we use a simplified transformer-like approach

llm_model = FinancialLLM(
    input_dim=len(available_features),
    hidden_dim=64,
    num_layers=2,
    output_dim=1
)

# Train simplified LLM
llm_history = llm_model.train(
    X_train_scaled, y_train,
    X_test_scaled, y_test,
    epochs=50,
    batch_size=64
)

llm_pred = (llm_model.predict(X_test_scaled) > 0.5).astype(int).flatten()

results['FinancialLLM'] = {
    'model': llm_model,
    'predictions': llm_pred,
    'accuracy': accuracy_score(y_test, llm_pred),
    'f1': f1_score(y_test, llm_pred),
    'training_history': llm_history
}

print(f" Accuracy: {results['FinancialLLM']['accuracy']:.3f}")
print(f" F1-Score: {results['FinancialLLM']['f1']:.3f}")

# ============================================================================
# 5. VISUALIZE RESULTS
# ============================================================================
print("\n5. VISUALIZING BASELINE RESULTS")
print("-" * 40)

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# Accuracy comparison
models = list(results.keys())
accuracies = [results[m]['accuracy'] for m in models]
f1_scores = [results[m]['f1'] for m in models]

x = np.arange(len(models))
width = 0.35

axes[0, 0].bar(x - width/2, accuracies, width, label='Accuracy', color='skyblue')
axes[0, 0].bar(x + width/2, f1_scores, width, label='F1-Score', color='lightcoral')
axes[0, 0].set_xlabel('Model')
axes[0, 0].set_ylabel('Score')
axes[0, 0].set_title('Model Performance Comparison')
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(models, rotation=45)
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Confusion matrix for best model
best_model_name = max(results, key=lambda x: results[x]['f1'])
best_pred = results[best_model_name]['predictions']

cm = confusion_matrix(y_test, best_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])
axes[0, 1].set_title(f'Confusion Matrix - {best_model_name}')
axes[0, 1].set_xlabel('Predicted')
axes[0, 1].set_ylabel('Actual')

# ROC curves (simplified)
from sklearn.metrics import roc_curve, auc

for i, (model_name, model_results) in enumerate(results.items()):
    if 'probabilities' in model_results:
        fpr, tpr, _ = roc_curve(y_test, model_results['probabilities'])
        roc_auc = auc(fpr, tpr)
        axes[0, 2].plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})')

axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.3)
axes[0, 2].set_xlabel('False Positive Rate')
axes[0, 2].set_ylabel('True Positive Rate')
axes[0, 2].set_title('ROC Curves')
axes[0, 2].legend()
axes[0, 2].grid(True, alpha=0.3)

# Feature importance for tree-based models
for i, (model_name, model_results) in enumerate(results.items()):
    if hasattr(model_results['model'], 'feature_importances_'):
        importances = model_results['model'].feature_importances_
        indices = np.argsort(importances)[-10:] # Top 10
        
        axes[1, 0].barh(range(len(indices)), importances[indices], 
                       label=model_name, alpha=0.6)
        
axes[1, 0].set_yticks(range(len(indices)))
axes[1, 0].set_yticklabels([available_features[i] for i in indices])
axes[1, 0].set_xlabel('Importance')
axes[1, 0].set_title('Feature Importances (Top 10)')
axes[1, 0].legend()

# LLM training history
if 'FinancialLLM' in results:
    history = results['FinancialLLM']['training_history']
    axes[1, 1].plot(history['train_loss'], label='Train Loss')
    axes[1, 1].plot(history['val_loss'], label='Val Loss')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Loss')
    axes[1, 1].set_title('LLM Training History')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

# Prediction distribution
axes[1, 2].hist(results[best_model_name]['probabilities'], bins=50, alpha=0.7)
axes[1, 2].axvline(x=0.5, color='r', linestyle='--', alpha=0.5)
axes[1, 2].set_xlabel('Prediction Probability')
axes[1, 2].set_ylabel('Frequency')
axes[1, 2].set_title(f'Prediction Distribution - {best_model_name}')

plt.tight_layout()
plt.savefig('../results/baseline_model_results.png', dpi=150, bbox_inches='tight')
plt.show()

# ============================================================================
# 6. ETHICAL METRICS EVALUATION
# ============================================================================
print("\n6. ETHICAL METRICS EVALUATION")
print("-" * 40)

ethical_metrics = EthicalMetricsCalculator()

# Calculate ethical metrics for each model
ethical_results = {}
for model_name, model_results in results.items():
    ethical_report = ethical_metrics.calculate(
        predictions=model_results['predictions'],
        probabilities=model_results.get('probabilities', None),
        features=X_test_scaled,
        model_type='baseline'
    )
    ethical_results[model_name] = ethical_report

# Display ethical metrics
print("\nEthical Metrics Summary:")
print("-" * 60)
print(f"{'Model':<15} {'Fairness':<10} {'Bias':<10} {'Privacy':<10} {'Explainability':<15}")
print("-" * 60)

for model_name, metrics in ethical_results.items():
    print(f"{model_name:<15} "
          f"{metrics.get('fairness_score', 0):<10.3f} "
          f"{metrics.get('bias_score', 0):<10.3f} "
          f"{metrics.get('privacy_score', 0):<10.3f} "
          f"{metrics.get('explainability_score', 0):<15.3f}")

# ============================================================================
# 7. SAVE BASELINE RESULTS
# ============================================================================
print("\n7. SAVING BASELINE RESULTS")
print("-" * 40)

results_dir = Path("../results/models/baseline")
results_dir.mkdir(parents=True, exist_ok=True)

# Save model performances
performance_df = pd.DataFrame({
    'Model': models,
    'Accuracy': accuracies,
    'F1_Score': f1_scores,
    'Best_Model': [m == best_model_name for m in models]
})

performance_df.to_csv(results_dir / "baseline_performance.csv", index=False)
print(f"✓ Saved performance metrics to: {results_dir}/baseline_performance.csv")

# Save ethical metrics
ethical_df = pd.DataFrame(ethical_results).T
ethical_df.to_csv(results_dir / "ethical_metrics.csv")
print(f"✓ Saved ethical metrics to: {results_dir}/ethical_metrics.csv")

# Save predictions for later comparison
predictions_data = {
    'actual': y_test,
    'timestamp': test_data['timestamp'].values[:len(y_test)]
}

for model_name, model_results in results.items():
    predictions_data[f'{model_name.lower()}_pred'] = model_results['predictions']
    if 'probabilities' in model_results:
        predictions_data[f'{model_name.lower()}_proba'] = model_results['probabilities']

predictions_df = pd.DataFrame(predictions_data)
predictions_df.to_parquet(results_dir / "baseline_predictions.parquet")
print(f"✓ Saved predictions to: {results_dir}/baseline_predictions.parquet")

print("\n" + "=" * 80)
print("BASELINE MODELING COMPLETE")
print("=" * 80)
print(f"✓ Trained {len(results)} baseline models")
print(f"✓ Best model: {best_model_name} (F1: {results[best_model_name]['f1']:.3f})")
print(f"✓ All ethical metrics calculated and stored")
print(f"✓ Baseline established for QGAN comparison")
print(f"✓ Data Minimization: Used only {len(available_features)} features")

